{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Spell Correction for Text preprocessing\n",
    "\n",
    "This is an ongoing improvement on the core implemenation of the Spell <br>checking algorithm created By Peter Norvig\n",
    "for more information on [modeling basicss](https://nbviewer.jupyter.org/url/norvig.com/ipython/How%20to%20Do%20Things%20with%20Words.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.metrics import edit_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'Basic_Processing.ipynb',\n",
       " 'data',\n",
       " 'HTML - Text Extraction.ipynb',\n",
       " 'Loading_from_Filesources.ipynb',\n",
       " 'Spell_correction.ipynb',\n",
       " 'Stemming_Lemmatizing_Words.ipynb',\n",
       " 'Untitled.ipynb',\n",
       " 'Working_with_sentences.ipynb']"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implementing a basic spell checker from Peter Norvig\n",
    "import re\n",
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "# Tokenize the words\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "WORDS = Counter(words(open('./data/big.txt', 'r').read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check_spelling(text):\n",
    "    \"\"\"\n",
    "    Spell corrects a text for the English language. \n",
    "    \n",
    "    Given a string of Text and a trained FreqDict of a given language, the model\n",
    "    tries to Spell correct the sentences keeping original Formatting and Casing in place.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    \n",
    "    def splits(word):\n",
    "        \"Return a list of all possible (first, rest) pairs that comprise word.\"\n",
    "        return [(word[:i], word[i:]) for i in range(len(word)+1)]\n",
    "    \n",
    "    def correct(word):\n",
    "        \"Find the best spelling correction for this word.\"\n",
    "        # Prefer edit distance 0, then 1, then 2; otherwise default to word itself.\n",
    "        candidates = (known(edits0(word)) or \n",
    "                      known(edits1(word)) or \n",
    "                      known(edits2(word)) or \n",
    "                      [word])\n",
    "        return max(candidates, key=WORDS.get)\n",
    "    \n",
    "    def correct_text(text):\n",
    "        \"Correct all the words within a text, returning the corrected text.\"\n",
    "        return re.sub('[a-zA-Z]+', correct_match, text)\n",
    "    \n",
    "    def correct_match(match):\n",
    "        \"Spell-correct word in match, and preserve proper upper/lower/title case.\"\n",
    "        word = match.group()\n",
    "        return case_of(word)(correct(word.lower()))\n",
    "    \n",
    "    def case_of(text):\n",
    "        \"Return the case-function appropriate for text: upper, lower, title, or just str.\"\n",
    "        return (str.upper if text.isupper() else\n",
    "                str.lower if text.islower() else\n",
    "                str.title if text.istitle() else\n",
    "                str)\n",
    "    \n",
    "    def candidates(word):\n",
    "        \"Generate probable spelling corrections for word\"\n",
    "        return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "    \n",
    "    def known(words):\n",
    "        \"The subset of 'words' that appear in the dictionary of WORDS\"\n",
    "        return {w for w in words if w in WORDS}\n",
    "    \n",
    "    def edits0(word):\n",
    "        return {word}\n",
    "    \n",
    "    def edits1(word):\n",
    "        \"Return all strings that are one edit away from this word.\"\n",
    "        pairs      = splits(word)\n",
    "        deletes    = [a+b[1:]           for (a, b) in pairs if b]\n",
    "        transposes = [a+b[1]+b[0]+b[2:] for (a, b) in pairs if len(b) > 1]\n",
    "        replaces   = [a+c+b[1:]         for (a, b) in pairs for c in alphabet if b]\n",
    "        inserts    = [a+c+b             for (a, b) in pairs for c in alphabet]\n",
    "        return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "    def edits2(word):\n",
    "        \"All edits that are two edits away from word\"\n",
    "        return {e2 for e1 in edits1(word) for e2 in edits1(e1)}\n",
    "    \n",
    "    return correct_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Spelling'"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_spelling('Speling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 79809),\n",
       " ('of', 40024),\n",
       " ('and', 38312),\n",
       " ('to', 28765),\n",
       " ('in', 22023),\n",
       " ('a', 21124),\n",
       " ('that', 12512),\n",
       " ('he', 12401),\n",
       " ('was', 11410),\n",
       " ('it', 10681)]"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WORDS.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Spelling Errors IN something. Whatever; unusual mistakes?'"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"Speling Errurs IN somethink. Whutever; unusuel misteakes?\"\n",
    "check_spelling(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the joint propability of a given sentence\n",
    "\n",
    "We should be able to compute the probability of a word, P(w)P(w). We do that with the function pdist, which takes as input a Counter (hat is, a bag of words) and returns a function that acts as a probability distribution over all possible words. In a probability distribution the probability of each word is between 0 and 1, and the sum of the probabilities is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pdist(counter):\n",
    "    \"Calculates the prior propability of a given Word\"\n",
    "    N = sum(counter.values())\n",
    "    return lambda x: counter[x]/N\n",
    "\n",
    "def product(nums):\n",
    "    \"Multiply the numbers together, like sum but with multiply\"\n",
    "    result = 1\n",
    "    for x in nums:\n",
    "        result *= x\n",
    "    return result\n",
    "\n",
    "Pword = pdist(WORDS)\n",
    "\n",
    "def Pwords(words):\n",
    "    \"Propability of word\"\n",
    "    return product(Pword(w) for w in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.870225938180123e-11, 8.233100124308226e-16, 0.0]"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tests = ['this is a test', \n",
    "         'this is a unusual test',\n",
    "         'this is a neverbeforeseen test']\n",
    "\n",
    "[Pwords(words(text)) for text in tests]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using sentence propability count to solve Word Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make one segmentation, into a first word and remaining characters. If we assume words are independent then we can maximize the probability of the first word adjoined to the best segmentation of the remaining characters.\n",
    "\n",
    "assert segment('choosespain') == ['choose', 'spain']\n",
    "\n",
    "segment('choosespain') ==\n",
    "   max(Pwords(['c'] + segment('hoosespain')),<br>\n",
    "       Pwords(['ch'] + segment('oosespain')),<br>\n",
    "       Pwords(['cho'] + segment('osespain')),<br>\n",
    "       Pwords(['choo'] + segment('sespain')),<br>\n",
    "       ...<br>\n",
    "       Pwords(['choosespain'] + segment('')))<br>\n",
    "       \n",
    "<br><br>\n",
    "To make this somewhat efficient, we need to avoid re-computing the segmentations of the remaining characters. This can be done explicitly by dynamic programming or implicitly with memoization. Also, we shouldn't consider all possible lengths for the first word; we can impose a maximum length. What should it be? A little more than the longest word seen so far.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def memo(f):\n",
    "    \"Memoize function f, whose args must all be hashable\"\n",
    "    cache = {}\n",
    "    def fmemo(*args):\n",
    "        if args not in cache:\n",
    "            cache[args] = f(*args)\n",
    "        return cache[args]\n",
    "    fmemo.cache = cache\n",
    "    return fmemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(w) for w in WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splits(text, start=0, L=20):\n",
    "    \"Returns a list of all (first, rest) pairs: start <= len(first) <= L\"\n",
    "    return [(text[:i], text[i:])  for i in range(start, min(len(text), L)+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('r', 'eallylongtextwithacoupleofwords'),\n",
       " ('re', 'allylongtextwithacoupleofwords'),\n",
       " ('rea', 'llylongtextwithacoupleofwords'),\n",
       " ('real', 'lylongtextwithacoupleofwords'),\n",
       " ('reall', 'ylongtextwithacoupleofwords'),\n",
       " ('really', 'longtextwithacoupleofwords'),\n",
       " ('reallyl', 'ongtextwithacoupleofwords'),\n",
       " ('reallylo', 'ngtextwithacoupleofwords'),\n",
       " ('reallylon', 'gtextwithacoupleofwords'),\n",
       " ('reallylong', 'textwithacoupleofwords')]"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits('reallylongtextwithacoupleofwords', 1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@memo\n",
    "def segment(text):\n",
    "    \"Return a list of words that is the most probable segmentation of text\"\n",
    "    if not text:\n",
    "        return []\n",
    "    else:\n",
    "        candidates = ([first] + segment(rest) for (first, rest) in splits(text,1))\n",
    "        return max(candidates, key=Pwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['when',\n",
       " 'in',\n",
       " 'the',\n",
       " 'course',\n",
       " 'of',\n",
       " 'human',\n",
       " 'events',\n",
       " 'it',\n",
       " 'becomes',\n",
       " 'necessary',\n",
       " 'for',\n",
       " 'one',\n",
       " 'people',\n",
       " 'to',\n",
       " 'dissolve',\n",
       " 'the',\n",
       " 'political',\n",
       " 'bands',\n",
       " 'which',\n",
       " 'have',\n",
       " 'connected',\n",
       " 'them',\n",
       " 'with',\n",
       " 'another',\n",
       " 'and',\n",
       " 'to',\n",
       " 'assume',\n",
       " 'among',\n",
       " 'the',\n",
       " 'powers',\n",
       " 'of',\n",
       " 'the',\n",
       " 'earth',\n",
       " 'the',\n",
       " 'separate',\n",
       " 'and',\n",
       " 'equal',\n",
       " 'station',\n",
       " 'to',\n",
       " 'which',\n",
       " 'the',\n",
       " 'laws',\n",
       " 'of',\n",
       " 'nature',\n",
       " 'and',\n",
       " 'of',\n",
       " 'natures',\n",
       " 'god',\n",
       " 'entitle',\n",
       " 'them',\n",
       " 's']"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment('wheninthecourseofhumaneventsitbecomesnecessaryforonepeople' +\n",
    "        'todissolvethepoliticalbandswhichhaveconnectedthemwithanother' +\n",
    "        'andtoassumeamongthepowersoftheearththeseparateandequalstation' +\n",
    "        'towhichthelawsofnatureandofnaturesgodentitlethems')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing / Good-Turing Smoothing Hack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'test',\n",
       " 'of',\n",
       " 'segment',\n",
       " 'at',\n",
       " 'i',\n",
       " 'on',\n",
       " 'of',\n",
       " 'along',\n",
       " 'sequence',\n",
       " 'of',\n",
       " 'words']"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test case\n",
    "segment('thisisatestofsegmentationofalongsequenceofwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two problems:\n",
    "\n",
    "First, we don't have a clear model of the unknown words. We just say \"unknown\" but we don't distinguish likely unknown from unlikely unknown. For example, is a 8-character unknown more likely than a 20-character unknown?\n",
    "\n",
    "Second, we don't take into account evidence from parts of the unknown. For example, \"unglobulate\" versus \"zxfkogultae\".\n",
    "\n",
    "For our next approach, Good - Turing smoothing re-estimates the probability of zero-count words, based on the probability of one-count words (and can also re-estimate for higher-number counts, but that is less interesting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "singletons = (w for w in WORDS if WORDS[w] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count1 = {word : count for word,count in zip(singletons, map(len, singletons))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1af8ce9cba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(list(lengths), bins=len(set(lenghts)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pdist_good_turing_hack(counter, onecounter, base=1/26., prior=1e-8):\n",
    "    \"\"\"The probability of word, given evidence from the counter.\n",
    "    For unknown words, look at the one-counts from onecounter, based on length.\n",
    "    This gets ideas from Good-Turing, but doesn't implement all of it.\n",
    "    prior is an additional factor to make unknowns less likely.\n",
    "    base is how much we attenuate probability for each letter beyond longest.\"\"\"\n",
    "    N = sum(counter.values())\n",
    "    N2 = sum(onecounter.values())\n",
    "    lengths = map(len, [w for w in onecounter if onecounter[w] == 1])\n",
    "    ones = Counter(lengths)\n",
    "    longest = max(ones)\n",
    "    return (lambda word: \n",
    "            counter[word] / N if (word in counter) \n",
    "            else prior * (ones[len(word)] / N2 or \n",
    "                          ones[longest] / N2 * base ** (len(word)-longest)))\n",
    "\n",
    "# Redefine P1w\n",
    "WORDS = pdist_good_turing_hack(count1, WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'test',\n",
       " 'of',\n",
       " 'segment',\n",
       " 'at',\n",
       " 'i',\n",
       " 'on',\n",
       " 'of',\n",
       " 'along',\n",
       " 'sequence',\n",
       " 'of',\n",
       " 'words']"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment.cache.clear()\n",
    "segment('thisisatestofsegmentationofalongsequenceofwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.401058637396523e-11\n"
     ]
    }
   ],
   "source": [
    "print(P1w('fransico'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
