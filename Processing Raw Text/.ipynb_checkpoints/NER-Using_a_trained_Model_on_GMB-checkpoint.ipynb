{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an NER Extractor on the Groningen MEANING BANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'O': 1146068, 'geo': 58388, 'org': 48094, 'per': 44254, 'tim': 34789, 'gpe': 20680, 'art': 867, 'eve': 709, 'nat': 300})\n"
     ]
    }
   ],
   "source": [
    "ner_tags = collections.Counter()\n",
    "\n",
    "corpus_root = \"./models/gmb-2.2.0/data/\"\n",
    "\n",
    "for root, dirs, files in os.walk(corpus_root):\n",
    "    for filename in files:\n",
    "        if filename.endswith('.tags'):\n",
    "            with open(os.path.join(root, filename), 'rb') as file_handle:\n",
    "                file_content = file_handle.read().decode('utf-8').strip()\n",
    "                annotated_sentences = file_content.split('\\n\\n') # Split the sentences according to markup\n",
    "                for annotated_sentence in annotated_sentences:\n",
    "                    annotated_tokens = [seq for seq in annotated_sentence.split('\\n')]\n",
    "                    \n",
    "                    standard_form_tokens = []\n",
    "                    \n",
    "                    for idx, annotated_token in enumerate(annotated_tokens):\n",
    "                        annotations = annotated_token.split('\\t') # Split annotations\n",
    "                        word, tag, ner = annotations[0], annotations[1], annotations[3]\n",
    "                        \n",
    "                        # To get only the primary categories\n",
    "                        if ner != 'O':\n",
    "                            ner = ner.split('-')[0]\n",
    "                        \n",
    "                        ner_tags[ner] += 1\n",
    "\n",
    "print(ner_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Corpus is annotated with the following Entity Types:\n",
    "\n",
    "* geo = Geographical Entity\n",
    "* org = Organization\n",
    "* per = Person\n",
    "* gpe = Geopolitical Entity\n",
    "* tim = Time indicator\n",
    "* art = Artifact\n",
    "* eve = Event\n",
    "* nat = Natural Phenomenon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our own System on the Wordbank Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we create a Feature Extractor to prepare the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def features(tokens, index, history):\n",
    "    \"\"\"\n",
    "    `tokens`  = a POS-tagged sentence [(w1, t1), ...]\n",
    "    `index`   = the index of the token we want to extract features for\n",
    "    `history` = the previous predicted IOB tags\n",
    "    \"\"\"\n",
    "    \n",
    "    # Init the stemmer\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    \n",
    "    # Pad the sequence with placeholders\n",
    "    tokens = [('[START2]', '[START2]'), ('[START1]', '[START1]')] + list(tokens) + [('[END1]', '[END1]'), ('[END2]', '[END2]')]\n",
    "    history = ['[START2]', '[START1]'] + list(history)\n",
    "    \n",
    "    \n",
    "    # Shift the indexes by 2\n",
    "    index += 2\n",
    "    \n",
    "    word, pos = tokens[index]\n",
    "    prevword, prevpos = tokens[index - 1]\n",
    "    prevprevword, prevprevpos =tokens[index - 2]\n",
    "    nextword, nextpos = tokens[index + 1]\n",
    "    nextnextword, nextnextpos = tokens[index + 2]\n",
    "    previob = history[index - 1]\n",
    "    contains_dash = '-' in word\n",
    "    contains_dot = '.' in word\n",
    "    allascii = all([True for c in word if c in string.ascii_lowercase])\n",
    "    \n",
    "    allcaps = word == word.capitalize()\n",
    "    capitalized = word[0] in string.ascii_uppercase\n",
    "    \n",
    "    prevallcaps = prevword == prevword.capitalize()\n",
    "    prevcapitalized = prevword[0] in string.ascii_uppercase\n",
    "    \n",
    "    nextallcaps = nextword == nextword.capitalize()\n",
    "    nextcapitalized = nextword[0] in string.ascii_uppercase\n",
    "    \n",
    "    return {\n",
    "        'word': word, \n",
    "        'lemma': stemmer.stem(word),\n",
    "        'pos': pos,\n",
    "        'all-ascii': allascii,\n",
    "        \n",
    "        'next-word': nextword,\n",
    "        'next-lemma': stemmer.stem(nextword),\n",
    "        'next-pos': nextpos,\n",
    "        \n",
    "        'next-next-word': nextnextword,\n",
    "        'nextnextpos': nextnextpos,\n",
    "        \n",
    "        'prev-word': prevword,\n",
    "        'prev-lemma': stemmer.stem(prevword),\n",
    "        'prev-pos': prevpos,\n",
    "        \n",
    "        'prev-prev-word': prevprevword,\n",
    "        'prev-prev-pos': prevprevpos,\n",
    "        \n",
    "        'prev-iob': previob,\n",
    "        \n",
    "        'contains-dash': contains_dash,\n",
    "        'contains-dot': contains_dot, \n",
    "        \n",
    "        'all-caps': allcaps,\n",
    "        'capitalized': capitalized,\n",
    "        \n",
    "        'prev-all-caps': prevallcaps, \n",
    "        'prev-capitalized': prevcapitalized,\n",
    "        \n",
    "        'next-all-caps': nextallcaps,\n",
    "        'next-capitalized': nextcapitalized,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we create a few utility functions to help in training `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_conll_iob(annotated_sentence):\n",
    "    \"\"\"\n",
    "    `annotated_sentence` = list of triplets [(w1, t1, iob1), ...]\n",
    "    Transform a pseudo-IOB notation: O, PERSON, PERSON, O, O, LOCATION, O\n",
    "    to proper IOB notation: O, B-PERSON, I-PERSON, O, O, B-LOCATION, O\n",
    "    \"\"\"\n",
    "    \n",
    "    proper_iob_tokens = []\n",
    "    for idx, annotated_token in enumerate(annotated_sentence):\n",
    "        tag, word, ner = annotated_token\n",
    "        \n",
    "        if ner != 'O':\n",
    "            if idx == 0:\n",
    "                ner = \"B-\" + ner\n",
    "            elif annotated_sentence[idx - 1][2] == ner:\n",
    "                ner = 'I-' + ner\n",
    "            else:\n",
    "                ner = \"B-\" + ner\n",
    "        proper_iob_tokens.append((tag, word, ner))\n",
    "    return proper_iob_tokens\n",
    "\n",
    "\n",
    "def read_gmb(corpus_root):\n",
    "    for root, dirs, files in os.walk(corpus_root):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".tags\"):\n",
    "                with open(os.path.join(root, filename), 'rb') as file_handle:\n",
    "                    file_content = file_handle.read().decode('utf-8').strip()\n",
    "                    annotated_sentences = file_content.split('\\n\\n')\n",
    "                    for annotated_sentence in annotated_sentences:\n",
    "                        annotated_tokens = [seq for seq in annotated_sentence.split('\\n') if seq]\n",
    " \n",
    "                        standard_form_tokens = []\n",
    " \n",
    "                        for idx, annotated_token in enumerate(annotated_tokens):\n",
    "                            annotations = annotated_token.split('\\t')\n",
    "                            word, tag, ner = annotations[0], annotations[1], annotations[3]\n",
    " \n",
    "                            if ner != 'O':\n",
    "                                ner = ner.split('-')[0]\n",
    " \n",
    "                            if tag in ('LQU', 'RQU'):   # Make it NLTK compatible\n",
    "                                tag = \"``\"\n",
    " \n",
    "                            standard_form_tokens.append((word, tag, ner))\n",
    " \n",
    "                        conll_tokens = to_conll_iob(standard_form_tokens)\n",
    " \n",
    "                        # Make it NLTK Classifier compatible - [(w1, t1, iob1), ...] to [((w1, t1), iob1), ...]\n",
    "                        # Because the classfier expects a tuple as input, first item input, second the class\n",
    "                        yield [((w, t), iob) for w, t, iob in conll_tokens]\n",
    " \n",
    "reader = read_gmb(corpus_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(('Thousands', 'NNS'), 'O'),\n",
       "  (('of', 'IN'), 'O'),\n",
       "  (('demonstrators', 'NNS'), 'O'),\n",
       "  (('have', 'VBP'), 'O'),\n",
       "  (('marched', 'VBN'), 'O'),\n",
       "  (('through', 'IN'), 'O'),\n",
       "  (('London', 'NNP'), 'B-geo'),\n",
       "  (('to', 'TO'), 'O'),\n",
       "  (('protest', 'VB'), 'O'),\n",
       "  (('the', 'DT'), 'O'),\n",
       "  (('war', 'NN'), 'O'),\n",
       "  (('in', 'IN'), 'O'),\n",
       "  (('Iraq', 'NNP'), 'B-geo'),\n",
       "  (('and', 'CC'), 'O'),\n",
       "  (('demand', 'VB'), 'O'),\n",
       "  (('the', 'DT'), 'O'),\n",
       "  (('withdrawal', 'NN'), 'O'),\n",
       "  (('of', 'IN'), 'O'),\n",
       "  (('British', 'JJ'), 'B-gpe'),\n",
       "  (('troops', 'NNS'), 'O'),\n",
       "  (('from', 'IN'), 'O'),\n",
       "  (('that', 'DT'), 'O'),\n",
       "  (('country', 'NN'), 'O'),\n",
       "  (('.', '.'), 'O')],\n",
       " [(('Families', 'NNS'), 'O'),\n",
       "  (('of', 'IN'), 'O'),\n",
       "  (('soldiers', 'NNS'), 'O'),\n",
       "  (('killed', 'VBN'), 'O'),\n",
       "  (('in', 'IN'), 'O'),\n",
       "  (('the', 'DT'), 'O'),\n",
       "  (('conflict', 'NN'), 'O'),\n",
       "  (('joined', 'VBD'), 'O'),\n",
       "  (('the', 'DT'), 'O'),\n",
       "  (('protesters', 'NNS'), 'O'),\n",
       "  (('who', 'WP'), 'O'),\n",
       "  (('carried', 'VBD'), 'O'),\n",
       "  (('banners', 'NNS'), 'O'),\n",
       "  (('with', 'IN'), 'O'),\n",
       "  (('such', 'JJ'), 'O'),\n",
       "  (('slogans', 'NNS'), 'O'),\n",
       "  (('as', 'IN'), 'O'),\n",
       "  (('\"', '``'), 'O'),\n",
       "  (('Bush', 'NNP'), 'B-per'),\n",
       "  (('Number', 'NN'), 'O'),\n",
       "  (('One', 'CD'), 'O'),\n",
       "  (('Terrorist', 'NN'), 'O'),\n",
       "  (('\"', '``'), 'O'),\n",
       "  (('and', 'CC'), 'O'),\n",
       "  (('\"', '``'), 'O'),\n",
       "  (('Stop', 'VB'), 'O'),\n",
       "  (('the', 'DT'), 'O'),\n",
       "  (('Bombings', 'NNS'), 'O'),\n",
       "  (('.', '.'), 'O'),\n",
       "  (('\"', '``'), 'O')],\n",
       " [(('They', 'PRP'), 'O'),\n",
       "  (('marched', 'VBD'), 'O'),\n",
       "  (('from', 'IN'), 'O'),\n",
       "  (('the', 'DT'), 'O'),\n",
       "  (('Houses', 'NNS'), 'O'),\n",
       "  (('of', 'IN'), 'O'),\n",
       "  (('Parliament', 'NN'), 'O'),\n",
       "  (('to', 'TO'), 'O'),\n",
       "  (('a', 'DT'), 'O'),\n",
       "  (('rally', 'NN'), 'O'),\n",
       "  (('in', 'IN'), 'O'),\n",
       "  (('Hyde', 'NNP'), 'B-geo'),\n",
       "  (('Park', 'NNP'), 'I-geo'),\n",
       "  (('.', '.'), 'O')],\n",
       " [(('Police', 'NNS'), 'O'),\n",
       "  (('put', 'VBD'), 'O'),\n",
       "  (('the', 'DT'), 'O'),\n",
       "  (('number', 'NN'), 'O'),\n",
       "  (('of', 'IN'), 'O'),\n",
       "  (('marchers', 'NNS'), 'O'),\n",
       "  (('at', 'IN'), 'O'),\n",
       "  (('10,000', 'CD'), 'O'),\n",
       "  (('while', 'IN'), 'O'),\n",
       "  (('organizers', 'NNS'), 'O'),\n",
       "  (('claimed', 'VBD'), 'O'),\n",
       "  (('it', 'PRP'), 'O'),\n",
       "  (('was', 'VBD'), 'O'),\n",
       "  (('100,000', 'CD'), 'O'),\n",
       "  (('.', '.'), 'O')],\n",
       " [(('The', 'DT'), 'O'),\n",
       "  (('protest', 'NN'), 'O'),\n",
       "  (('comes', 'VBZ'), 'O'),\n",
       "  (('on', 'IN'), 'O'),\n",
       "  (('the', 'DT'), 'O'),\n",
       "  (('eve', 'NN'), 'O'),\n",
       "  (('of', 'IN'), 'O'),\n",
       "  (('the', 'DT'), 'O'),\n",
       "  (('annual', 'JJ'), 'O'),\n",
       "  (('conference', 'NN'), 'O'),\n",
       "  (('of', 'IN'), 'O'),\n",
       "  (('Britain', 'NNP'), 'B-geo'),\n",
       "  ((\"'s\", 'POS'), 'O'),\n",
       "  (('ruling', 'VBG'), 'O'),\n",
       "  (('Labor', 'NNP'), 'B-org'),\n",
       "  (('Party', 'NNP'), 'I-org'),\n",
       "  (('in', 'IN'), 'O'),\n",
       "  (('the', 'DT'), 'O'),\n",
       "  (('southern', 'JJ'), 'O'),\n",
       "  (('English', 'JJ'), 'B-gpe'),\n",
       "  (('seaside', 'NN'), 'O'),\n",
       "  (('resort', 'NN'), 'O'),\n",
       "  (('of', 'IN'), 'O'),\n",
       "  (('Brighton', 'NNP'), 'B-geo'),\n",
       "  (('.', '.'), 'O')],\n",
       " [(('The', 'DT'), 'O'),\n",
       "  (('party', 'NN'), 'O'),\n",
       "  (('is', 'VBZ'), 'O'),\n",
       "  (('divided', 'VBN'), 'O'),\n",
       "  (('over', 'IN'), 'O'),\n",
       "  (('Britain', 'NNP'), 'B-gpe'),\n",
       "  ((\"'s\", 'POS'), 'O'),\n",
       "  (('participation', 'NN'), 'O'),\n",
       "  (('in', 'IN'), 'O'),\n",
       "  (('the', 'DT'), 'O'),\n",
       "  (('Iraq', 'NNP'), 'B-geo'),\n",
       "  (('conflict', 'NN'), 'O'),\n",
       "  (('and', 'CC'), 'O'),\n",
       "  (('the', 'DT'), 'O'),\n",
       "  (('continued', 'JJ'), 'O'),\n",
       "  (('deployment', 'NN'), 'O'),\n",
       "  (('of', 'IN'), 'O'),\n",
       "  (('8,500', 'CD'), 'O'),\n",
       "  (('British', 'JJ'), 'B-gpe'),\n",
       "  (('troops', 'NNS'), 'O'),\n",
       "  (('in', 'IN'), 'O'),\n",
       "  (('that', 'DT'), 'O'),\n",
       "  (('country', 'NN'), 'O'),\n",
       "  (('.', '.'), 'O')],\n",
       " [(('The', 'DT'), 'O'),\n",
       "  (('London', 'NNP'), 'B-geo'),\n",
       "  (('march', 'NN'), 'O'),\n",
       "  (('came', 'VBD'), 'O'),\n",
       "  (('ahead', 'RB'), 'O'),\n",
       "  (('of', 'IN'), 'O'),\n",
       "  (('anti-war', 'JJ'), 'O'),\n",
       "  (('protests', 'NNS'), 'O'),\n",
       "  (('today', 'NN'), 'O'),\n",
       "  (('in', 'IN'), 'O'),\n",
       "  (('other', 'JJ'), 'O'),\n",
       "  (('cities', 'NNS'), 'O'),\n",
       "  ((',', ','), 'O'),\n",
       "  (('including', 'VBG'), 'O'),\n",
       "  (('Rome', 'NNP'), 'B-geo'),\n",
       "  ((',', ','), 'O'),\n",
       "  (('Paris', 'NNP'), 'B-geo'),\n",
       "  ((',', ','), 'O'),\n",
       "  (('and', 'CC'), 'O'),\n",
       "  (('Madrid', 'NNP'), 'B-geo'),\n",
       "  (('.', '.'), 'O')],\n",
       " [(('The', 'DT'), 'O'),\n",
       "  (('International', 'NNP'), 'B-org'),\n",
       "  (('Atomic', 'NNP'), 'I-org'),\n",
       "  (('Energy', 'NNP'), 'I-org'),\n",
       "  (('Agency', 'NNP'), 'I-org'),\n",
       "  (('is', 'VBZ'), 'O'),\n",
       "  (('to', 'TO'), 'O'),\n",
       "  (('hold', 'VB'), 'O'),\n",
       "  (('second', 'JJ'), 'O'),\n",
       "  (('day', 'NN'), 'O'),\n",
       "  (('of', 'IN'), 'O'),\n",
       "  (('talks', 'NNS'), 'O'),\n",
       "  (('in', 'IN'), 'O'),\n",
       "  (('Vienna', 'NNP'), 'B-geo'),\n",
       "  (('Wednesday', 'NNP'), 'B-tim'),\n",
       "  (('on', 'IN'), 'O'),\n",
       "  (('how', 'WRB'), 'O'),\n",
       "  (('to', 'TO'), 'O'),\n",
       "  (('respond', 'VB'), 'O'),\n",
       "  (('to', 'TO'), 'O'),\n",
       "  (('Iran', 'NNP'), 'B-gpe'),\n",
       "  ((\"'s\", 'POS'), 'O'),\n",
       "  (('resumption', 'NN'), 'O'),\n",
       "  (('of', 'IN'), 'O'),\n",
       "  (('low-level', 'JJ'), 'O'),\n",
       "  (('uranium', 'NN'), 'O'),\n",
       "  (('conversion', 'NN'), 'O'),\n",
       "  (('.', '.'), 'O')],\n",
       " [(('Iran', 'NNP'), 'B-gpe'),\n",
       "  (('this', 'DT'), 'O'),\n",
       "  (('week', 'NN'), 'O'),\n",
       "  (('restarted', 'VBD'), 'O'),\n",
       "  (('parts', 'NNS'), 'O'),\n",
       "  (('of', 'IN'), 'O'),\n",
       "  (('the', 'DT'), 'O'),\n",
       "  (('conversion', 'NN'), 'O'),\n",
       "  (('process', 'NN'), 'O'),\n",
       "  (('at', 'IN'), 'O'),\n",
       "  (('its', 'PRP$'), 'O'),\n",
       "  (('Isfahan', 'NNP'), 'B-geo'),\n",
       "  (('nuclear', 'JJ'), 'O'),\n",
       "  (('plant', 'NN'), 'O'),\n",
       "  (('.', '.'), 'O')],\n",
       " [(('Iranian', 'JJ'), 'B-gpe'),\n",
       "  (('officials', 'NNS'), 'O'),\n",
       "  (('say', 'VBP'), 'O'),\n",
       "  (('they', 'PRP'), 'O'),\n",
       "  (('expect', 'VBP'), 'O'),\n",
       "  (('to', 'TO'), 'O'),\n",
       "  (('get', 'VB'), 'O'),\n",
       "  (('access', 'NN'), 'O'),\n",
       "  (('to', 'TO'), 'O'),\n",
       "  (('sealed', 'JJ'), 'O'),\n",
       "  (('sensitive', 'JJ'), 'O'),\n",
       "  (('parts', 'NNS'), 'O'),\n",
       "  (('of', 'IN'), 'O'),\n",
       "  (('the', 'DT'), 'O'),\n",
       "  (('plant', 'NN'), 'O'),\n",
       "  (('Wednesday', 'NNP'), 'B-tim'),\n",
       "  ((',', ','), 'O'),\n",
       "  (('after', 'IN'), 'O'),\n",
       "  (('an', 'DT'), 'O'),\n",
       "  (('IAEA', 'NNP'), 'B-org'),\n",
       "  (('surveillance', 'NN'), 'O'),\n",
       "  (('system', 'NN'), 'O'),\n",
       "  (('begins', 'VBZ'), 'O'),\n",
       "  (('functioning', 'VBG'), 'O'),\n",
       "  (('.', '.'), 'O')]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(reader)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model and Save it for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import Iterable\n",
    "from nltk.tag import ClassifierBasedTagger\n",
    "from nltk.chunk import ChunkParserI\n",
    "\n",
    "class NamedEntityChunker(ChunkParserI):\n",
    "    \n",
    "    def __init__(self, train_sents, **kwargs):\n",
    "        assert isinstance(train_sents, Iterable)\n",
    "        \n",
    "        self.feature_detector = features\n",
    "        self.tagger = ClassifierBasedTagger(\n",
    "            train=train_sents,\n",
    "            feature_detector=features,\n",
    "            **kwargs)\n",
    "        \n",
    "    \n",
    "    def parse(self, tagged_sent):\n",
    "        chunks = self.tagger.tag(tagged_sent)\n",
    "        \n",
    "        # Transform the result from [((w1, t1), iob1), ...]\n",
    "        # to the prefered list of triplets format [(w1, t1, iob1), ...]\n",
    "        iob_triplets = [(w,t,c) for ((w,t),c) in chunks]\n",
    "        \n",
    "        # Transform the list of triplet to nltk.Tree format\n",
    "        return conlltags2tree(iob_triplets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's build the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training Samples:  55809\n",
      "# Test Samples:  6201\n"
     ]
    }
   ],
   "source": [
    "reader = read_gmb(corpus_root)\n",
    "data = list(reader)\n",
    "training_samples = data[:int(len(data) * 0.9)]\n",
    "test_samples = data[int(len(data) * 0.9):]\n",
    "\n",
    "\n",
    "print('# Training Samples: ', len(training_samples))\n",
    "print('# Test Samples: ', len(test_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chunker = NamedEntityChunker(training_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (per George/NNP Bush/NNP)\n",
      "  visited/VBD\n",
      "  (geo Germany/NNP)\n",
      "  this/DT\n",
      "  Monday/NNP)\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "print(chunker.parse(pos_tag(word_tokenize(\"George Bush visited Germany this Monday\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store the Model\n",
    "with open('./models/001_production/GMB_NERTagger.pkl', 'wb') as file:\n",
    "    pickle.dump(chunker, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
